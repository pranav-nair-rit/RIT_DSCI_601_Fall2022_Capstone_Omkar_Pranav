{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5802b7e",
   "metadata": {},
   "source": [
    "# Real World Data_Mining for GitHub Commits\n",
    "\n",
    "## We have used the GitHub API in this section to get the all closed Commits form open source projects\n",
    "\n",
    "## We will be implementing the HyperGraph Approach in this Project \n",
    "### In this Approach the inputs required are:\n",
    "#### 1. PR Number\n",
    "#### 2. PR Developer(Name)\n",
    "#### 3. PR Reviewers\n",
    "#### 4. PR Reviewers Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prameters used in this sections :\n",
    "#     Token : this is s github token to use the github api and extract the data using API\n",
    "#     Reopos : open source project name and repo name form which data needs to be extracted\n",
    "#     Headers : autorization for using github API\n",
    "#     Paramers : request per page (max PR per page is 100)\n",
    "\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import tqdm\n",
    "params = {'per_page':100}\n",
    "repos = {\"scikit-learn\":\"scikit-learn\"}\n",
    "token = \"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\" \n",
    "headers = {\n",
    "    \"authorization\" : f\"Bearer {token}\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "for key, value in tqdm.tqdm(repos.items()): # Lopping on all the repos from which we want to extract the PR data\n",
    "    another_page = True\n",
    "    api = f\"https://api.github.com/repos/{key}/{value}/pulls?state=all\" #api link to get the data in json\n",
    "    while another_page: #This conditions checks if the next page is present to extact the PRs\n",
    "        r = requests.get(api,headers=headers, params=params) #Getting the PR from the link\n",
    "        print(f\"{key}: \",r)\n",
    "        json_response = json.loads(r.text) # saving the json response from api in variable (This is contacins PRs in single page)\n",
    "        results.append(json_response) # appending the json file in list \n",
    "        if 'next' in r.links: #check if there is another page of organisations\n",
    "            api = r.links['next']['url'] # takes the link of next PR page\n",
    "        else:\n",
    "            another_page=False\n",
    "merged_prs = [] # List to get all merged PRs from closed PR\n",
    "\n",
    "for page in results: # Looping over all the PRs to check if they were merged or not\n",
    "    for pr in page: \n",
    "        if pr[\"merged_at\"]!=None: # If merged date is present in the data we are daving that PR in a list\n",
    "            merged_prs.append(pr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b13d16",
   "metadata": {},
   "source": [
    "### 1. In this project we are only taking the closed and merged PR.\n",
    "### 2. Reason behind doing this is getting the reviews for all of them. Open TR dont have a reviewers and comments on them so such null data will not be of any use for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting all the \n",
    "numbers = []\n",
    "\n",
    "for pr in merged_prs: # Looping over all closed and merged PR to get PR_ID\n",
    "    number = pr.get(\"number\")\n",
    "    numbers.append(number) # Saving the PR ids in a list for further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef52e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import tqdm\n",
    "params = {'per_page':100} # PRs per page\n",
    "token = \"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\" #Token for accessing the API\n",
    "headers = {\n",
    "    \"authorization\" : f\"Bearer {token}\"\n",
    "}\n",
    "reviewers = []\n",
    "for num in numbers[:]:\n",
    "    r_api = f\"https://api.github.com/repos/scikit-learn/scikit-learn/pulls/{num}/reviews\" # API to get the reviewers and their comments\n",
    "    r = requests.get(r_api,headers=headers)\n",
    "    print(f\"{key}: \",r)\n",
    "    json_response = r.json()\n",
    "    reviewers.append((num, json_response))\n",
    "\n",
    "pr_creators = []\n",
    "\n",
    "for pr in merged_prs:\n",
    "    number = pr['number']\n",
    "    creator = pr['user']['login']\n",
    "    pr_creators.append((number, creator))\n",
    "\n",
    "with open(\"reviewers.bin\", 'wb') as f:\n",
    "    pickle.dump(reviewers, f) # Saving all the data using pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff9439f",
   "metadata": {},
   "source": [
    "### 1. From the above code we have got the reviewers and there comments but also There are multiple data along with that which are not required to us \n",
    "### 2. In the following cell we will extract only those data which are required and saving it into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"reviewers.bin\", 'rb') as f:\n",
    "    rev = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e870f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# df = pd.DataFrame(columns=['PR_ID', 'REVIEWER','REVIEWER_COMMENTS'])\n",
    "count = 0\n",
    "list_all_data = list()\n",
    "for i in range(0,len(rev)):\n",
    "    count = count +1\n",
    "    rev_set = set()\n",
    "    comments_dict = dict()\n",
    "    for j in range(0,len(rev[i][1])):\n",
    "        if(rev[i][1][j].get(\"user\") != None):\n",
    "            rev_set.add(rev[i][1][j].get(\"user\").get(\"login\"))\n",
    "            if rev[i][1][j].get(\"user\").get(\"login\") in comments_dict.keys():\n",
    "                comments = comments_dict.get(rev[i][1][j].get(\"user\").get(\"login\"))\n",
    "                comments.append(rev[i][1][j].get(\"body\"))\n",
    "                comments_dict[rev[i][1][j].get(\"user\").get(\"login\")] = comments\n",
    "            else:\n",
    "                list_comment = list()\n",
    "                list_comment.append(rev[i][1][j].get(\"body\"))\n",
    "                comments_dict[rev[i][1][j].get(\"user\").get(\"login\")] = list_comment\n",
    "            \n",
    "#     print(rev[i][0])\n",
    "#     print('\\n')\n",
    "#     print(rev_set)\n",
    "#     print('\\n')\n",
    "#     print(comments_dict)\n",
    "#     print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "    list_all_data.append([rev[i][0],rev_set,comments_dict])\n",
    "    \n",
    "    print(count)\n",
    "    \n",
    "# #     new_df = pd.DataFrame([rev[i][0], rev_set, comments_dict], columns = [\"PR_ID\", \"REVIEWER\", \"REVIEWER_COMMENTS\"])\n",
    "# #     df = pd.concat([new_df])   \n",
    "\n",
    "df3 = pd.DataFrame(list_all_data, columns=['PR_ID', 'REVIEWER', 'REVIEWER_COMMENTS'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f65d7c",
   "metadata": {},
   "source": [
    "## Finally we have extracted the data which are requied for building our hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e39f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
